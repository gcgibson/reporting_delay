{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to forecast using estimates from the delay distribution we develop the following notation. \n",
    "\n",
    "\n",
    "Suppose we have a time series $Y_1,Y_2,...,Y_n$ indexed by a sequence of time values $t_1,t_2,...,t_n$. We adopt the forecasting notation of Reich et. al (2018) in order to evaluate targets across models. \n",
    "\n",
    "Suppose we are interested in forecasting $Y_j$ at time $t_j$. Because of the delay nature of the data we only observe **complete** data up till time $t_j - L$ for a fixed L. \n",
    "\n",
    "\n",
    "For example, if $t_j$ is \"now\" then $t_j - L$ is the last $t^*$ for which all possible delay values $t^*,t^*+1, ....,+ t^*+L$ are completely observed. \n",
    "\n",
    "We define the following prediction $z(t_j - L +k | t_j)$ to be the *k-step* ahead prediction relative to time $t_j$. We define the MSE of model $M$ to be \n",
    "\n",
    "$$MSE_{t_j}(M) = \\frac{1}{L}\\sum_{k=0}^L [z(t_j - L +k | t_j) - Y_{$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jan  6 17:12:30 2018\n",
    "\n",
    "@author: gcgibson\n",
    "\"\"\"\n",
    "\n",
    "import scipy.stats\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.random import random\n",
    "import math\n",
    "from pydlm import dlm, trend, seasonality, dynamic, autoReg, longSeason\n",
    "\n",
    "def observation_function(time_series_at_t,t,D,particle,params):\n",
    "    \n",
    "    tmp =  scipy.stats.poisson.pmf(time_series_at_t,np.exp(particle))\n",
    "    if math.isnan(tmp):\n",
    "        tmp = 0\n",
    "    return tmp\n",
    "\n",
    "def transition_function(particles,params):\n",
    "    particles[:,0]  += np.random.normal(0,1,len(particles))\n",
    "    return particles\n",
    "\n",
    "def expected_value_observation_function(p):\n",
    "    return np.exp(p)\n",
    "\n",
    "def expected_value_transition_function(p):\n",
    "    return p\n",
    "\n",
    "def create_uniform_particles( N,state_space_dimension):\n",
    "    particles  = np.random.normal(0,1 , size=(N,state_space_dimension))\n",
    "    return particles\n",
    "\n",
    "def predict(particles,t,params):\n",
    "    particles = transition_function(particles,params)\n",
    "    return particles\n",
    "\n",
    "\n",
    "def update(particles, weights,ts,t,D,params):\n",
    "    weights.fill(1.)\n",
    "    for p in range(len(particles)):\n",
    "        weights[p] *= observation_function(ts[t],t,D,particles[p],params)\n",
    "    weights += 1.e-300\n",
    "    return weights/sum(weights)  \n",
    "\n",
    "\n",
    "\n",
    "def neff(weights):\n",
    "    return 1. / np.sum(np.square(weights))\n",
    "\n",
    "\n",
    "def estimate(particles, weights):\n",
    "    \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "    pos = particles[:, 0]\n",
    "    mean = np.average(pos, weights=weights, axis=0)\n",
    "    var  = np.average((pos - mean)**2, weights=weights, axis=0)\n",
    "    return mean, var\n",
    "\n",
    "### VARIOUS RESAMPLING SCHEMES\n",
    "def multinomal_resample(weights):\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum[-1] = 1.  # avoid round-off errors\n",
    "    return np.searchsorted(cumulative_sum, random(len(weights)))\n",
    "\n",
    "\n",
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes]\n",
    "    weights[:] = weights[indexes]\n",
    "    weights.fill(1.0 / len(weights))\n",
    "    return particles,weights\n",
    "\n",
    "def stratified_resample(weights):\n",
    "    N = len(weights)\n",
    "    # make N subdivisions, chose a random position within each one\n",
    "    positions = (random(N) + range(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes\n",
    "\n",
    "\n",
    "\n",
    "def run_pf(time_series,N,state_space_dimension,D,params):\n",
    "    \n",
    "    particles = create_uniform_particles(N=N,state_space_dimension=state_space_dimension)\n",
    "    weights = np.zeros(N)    \n",
    "    xs = [] \n",
    "    ws = []\n",
    "    ws.append(weights)\n",
    "    for t in range(len(time_series)):\n",
    "        particles = predict(particles,t,params)       \n",
    "        # incorporate measurements\n",
    "        weights = update(particles, weights,time_series, t, D, params)\n",
    "        ws.append(weights)\n",
    "        #print (neff(weights),time_series[t],params)\n",
    "        indexes = stratified_resample(weights)\n",
    "        particles,weights = resample_from_index(particles, weights, indexes)\n",
    "        mu, var = estimate(particles, weights)\n",
    "        xs.append(mu.tolist())\n",
    "    return xs,particles,ws\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Suppose we are given data in the form \n",
    "\n",
    "n_{0,0} , n_{0,1} , n_{0,2}, n_{0,3}\n",
    "n_{1,0} , n_{1,1} , n_{1,2}, 0\n",
    "n_{2,0} , n_{2,1} , 0      , 0\n",
    "n_{3,0} , 0       , 0      , 0\n",
    "\n",
    "where T=3.\n",
    "\n",
    "Now suppose D=2, then we truncate this matrix as follows\n",
    "\n",
    "n_{0,0} , n_{0,1} , n_{0,2}, 0\n",
    "n_{1,0} , n_{1,1} , n_{1,2}, 0\n",
    "n_{2,0} , n_{2,1} , 0      , 0\n",
    "n_{3,0} , 0       , 0      , 0\n",
    "\n",
    "\n",
    "so for a setting of parameters (T=2,D=3) the reporting trapezoid is completely \n",
    "defined\n",
    "\n",
    "In order to get the N_{t,T}s we simply add up the rows\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SIMULATE = False\n",
    "D = 6\n",
    "if SIMULATE == False:\n",
    "    n_t_d = []\n",
    "    with open(\"province-biweek_with_delays.csv\") as f:\n",
    "        i = 0\n",
    "        for line in f.readlines():\n",
    "            if i > 0:\n",
    "                n_t_d.append(line.replace(\"\\n\",\"\").split(','))\n",
    "            i+=1\n",
    "    date_to_index = {}\n",
    "    \n",
    "    i = 0\n",
    "    for elm in n_t_d:\n",
    "        if len(elm[1]) == 1:\n",
    "            elm[1] = \"0\" + elm[1]\n",
    "        date_to_index[elm[0]+elm[1]] = i\n",
    "    \n",
    "        i+=1\n",
    "    \n",
    "    d_to_i = {}\n",
    "    i = 0\n",
    "    iter_ =  date_to_index.keys()\n",
    "    iter_.sort()\n",
    "    for key in iter_:\n",
    "    \n",
    "        d_to_i[key] = i\n",
    "        i+=1\n",
    "    \n",
    "    n_t = np.zeros((52-1,52-1))\n",
    "    \n",
    "    for elm in n_t_d:\n",
    "        try:\n",
    "            \n",
    "            sick_date = d_to_i[elm[0]+elm[1]]\n",
    "            report_date = d_to_i[elm[4] + elm[5]]\n",
    "            if int(elm[4] + elm[5]) < 201621 and int(elm[3]) == 1:\n",
    "                n_t[sick_date][report_date] += int(elm[3])\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    n_t_d = []\n",
    "    for row in range(len(n_t)):\n",
    "        #if len(n_t[row][row:row+D]) == D:\n",
    "            tmp = n_t[row][row:row+D].tolist()\n",
    "            while len(tmp) < D:\n",
    "                tmp += [0]\n",
    "            n_t_d.append(tmp)\n",
    "    \n",
    "    n_t_d = np.array(n_t_d)\n",
    "    \n",
    "    n_t_inf = np.sum(n_t_d,axis=1)\n",
    "\n",
    "else:\n",
    "    true_p = [.1,.3,.2,.15,.15,.1]\n",
    "    n_t_d = []\n",
    "    n_t_inf = []\n",
    "    states = []\n",
    "    tmp = np.random.normal(4,1 ,1)\n",
    "    states.append(tmp)\n",
    "    n_t_inf.append(np.random.poisson(np.exp(tmp))[0])\n",
    "    for i in range(1,10):\n",
    "        tmp = np.random.normal(states[i-1],1,1)\n",
    "        states.append(tmp)\n",
    "        n_t_inf.append(np.random.poisson(np.exp(tmp))[0])\n",
    "    for i in range(len(n_t_inf)):\n",
    "        n_t_d.append(np.random.multinomial(n_t_inf[i],true_p).tolist())\n",
    "        \n",
    "    \n",
    "\n",
    "train_n_t_d = n_t_d[:len(n_t_d)-D + 1]\n",
    "train_n_t_inf = n_t_inf[:len(n_t_d)-D + 1]\n",
    "\n",
    "test_n_t_d = n_t_d[len(n_t_d)- D + 1:]\n",
    "test_n_t_inf = n_t_inf[len(n_t_d)-D +1:]\n",
    "\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "## Delay Model\n",
    "\n",
    "DELAY_DIST = True\n",
    "if DELAY_DIST == True:\n",
    "\n",
    "    k = np.array(train_n_t_d).shape[1 ]\n",
    "    \n",
    "    with pm.Model() as multinom_test:\n",
    "        a = pm.Dirichlet('a', a=np.ones(k))\n",
    "        for i in range(len(train_n_t_d)):\n",
    "            data_pred = pm.Multinomial('data_pred_%s'% i, n=sum(train_n_t_d[i]), p=a, observed=train_n_t_d[i])\n",
    "        trace = pm.sample(50000, pm.Metropolis())\n",
    "        #trace = pm.sample(1000) # also works with NUTS\n",
    "    \n",
    "    pm.traceplot(trace[500:]);\n",
    "\n",
    "state_trajectories = []\n",
    "PF = False\n",
    "if  PF:\n",
    "    N = 10000\n",
    "    state_space_dimension = 1\n",
    " \n",
    "    params = []\n",
    "    means , particles, weights = run_pf(train_n_t_inf,N,state_space_dimension,D,params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### Interval Predictions\n",
    "    state_trajectories = [particles]\n",
    "    observation_trajectories = [np.exp(particles)]\n",
    "    for i in range(len(test_n_t_inf)):\n",
    "        tmp = expected_value_transition_function(state_trajectories[i-1])\n",
    "        observation_trajectories.append(expected_value_observation_function(tmp))\n",
    "        state_trajectories.append(tmp) \n",
    "    \n",
    "    state_trajectories = state_trajectories[1:]\n",
    "    ## MEAN\n",
    "    print (np.mean(observation_trajectories,axis=1))\n",
    "    ## QUANTILES \n",
    "    state_trajectories = np.array(state_trajectories).reshape((len(test_n_t_inf),-1))\n",
    "\n",
    "\n",
    "else:\n",
    "    myDLM = dlm(train_n_t_inf)\n",
    "    myDLM = myDLM + trend(1, name='lineTrend', w=1.0)\n",
    "    # add a 7 day seasonality with prior covariance 1.0\n",
    "    myDLM = myDLM + seasonality(52, name='7day', w=1.0)\n",
    "    # add a 3 step auto regression\n",
    "    myDLM = myDLM + autoReg(degree=2, data=train_n_t_inf, name='ar3', w=1.0)\n",
    "    myDLM.fit()\n",
    "    (predictMean, predictVar) = myDLM.predictN(N=D-1, date=myDLM.n-1)\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "for i in range(len(predictMean)):\n",
    "    samples = np.random.normal(predictMean[i],np.sqrt(predictVar[i]),4)\n",
    "    state_trajectories.append(samples)\n",
    "state_trajectories = np.array(state_trajectories)\n",
    "\n",
    "\n",
    "phat = trace['a'].mean(axis=0)\n",
    "from scipy.stats import binom\n",
    "\n",
    "\n",
    "\n",
    "myDLM.plot()\n",
    "\n",
    "##compute weighted trajectories \n",
    "\n",
    "weighted_trajectories = []\n",
    "for i in range(len(state_trajectories)):\n",
    "    tmp = []\n",
    "    samples = state_trajectories[i]\n",
    "    row_sum = sum(test_n_t_d[i])\n",
    "    q = sum(phat[:len(phat)-i-1])\n",
    "    for samp in samples:\n",
    "        btemp = binom.pmf(row_sum,samp,q)\n",
    "        if np.isnan(btemp):\n",
    "            tmp.append(0)\n",
    "        else:\n",
    "            tmp.append(btemp)\n",
    "        #print (row_sum,samp,q,btemp)\n",
    "    weighted_trajectories.append(tmp)\n",
    "weighted_trajectories = np.array(weighted_trajectories)\n",
    "\n",
    "\n",
    "for i in range(len(weighted_trajectories)):\n",
    "    weighted_trajectories[i] = weighted_trajectories[i]/sum(weighted_trajectories[i])\n",
    "\n",
    "\n",
    "###\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "max_indeces = np.argmax(weighted_trajectories,axis=1)\n",
    "max_point = []\n",
    "for i in range(len(max_indeces)):\n",
    "    max_point.append(state_trajectories[i][max_indeces[i]])\n",
    "\n",
    "\n",
    "print \"MSE ignoring delay\"\n",
    "print (mean_squared_error(np.average(state_trajectories,axis=1),test_n_t_inf))\n",
    "print \"MSE delay adjusted\"\n",
    "print (mean_squared_error(np.average(state_trajectories,weights = weighted_trajectories,axis=1),test_n_t_inf))\n",
    "print \"MSE taking most likely trajectory\"\n",
    "print (mean_squared_error(max_point,test_n_t_inf))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
