{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1804\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBEEQKcz/BzJTNfvDGatlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105cb38d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count = 0\n",
    "total_cases = []\n",
    "with open(\"total_data.csv\") as f:\n",
    "    for line in f.readlines():\n",
    "        #print (line)\n",
    "        if count ==0:\n",
    "            count +=1\n",
    "        else:\n",
    "            line_split = line.split(\",\")\n",
    "            \n",
    "            if line_split[2] == \"50\" and len(line_split) >=3:\n",
    "                total_cases.append(float(line_split[3]))\n",
    "                \n",
    "print (len(total_cases))\n",
    "proc_training_data_base = total_cases[(26*20):(len(total_cases) - 26*4)]\n",
    "plt.plot(proc_training_data_base)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (78,26) (78,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-4ca54e230065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bi-week\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_t_d\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_t_d\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (78,26) (78,) "
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            \n",
    "            \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rcParams['lines.linewidth']=1\n",
    "plt.rcParams['axes.facecolor']='w'\n",
    "\n",
    "n_t_d = []\n",
    "with open(\"province-biweek_with_delays.csv\") as f:\n",
    "    i = 0\n",
    "    for line in f.readlines():\n",
    "        if i > 0:\n",
    "            n_t_d.append(line.replace(\"\\n\",\"\").split(','))\n",
    "        i+=1\n",
    "\n",
    "n_t_d_1 = []\n",
    "\n",
    "for elm in n_t_d:\n",
    "    if elm[2] == \"50\":\n",
    "        n_t_d_1.append(elm)\n",
    "\n",
    "\n",
    "index_to_dates = {}\n",
    "dates_to_index = {}\n",
    "count = 0\n",
    "for i in [\"2014\",\"2015\",\"2016\"]:\n",
    "    for j in range(1,27):\n",
    "        index_to_dates[count] = str(i)+str(j)\n",
    "        dates_to_index[str(i)+str(j)] = count\n",
    "        count +=1\n",
    "        \n",
    "reporting_matrix = np.zeros((26*3,26*3))\n",
    "\n",
    "for elm in n_t_d_1:\n",
    "    try:\n",
    "        sick_date = elm[0]+elm[1]\n",
    "        report_date = elm[-2] + elm[-1]\n",
    "        cases = elm[3]\n",
    "        reporting_matrix_row = dates_to_index[sick_date] \n",
    "        reporting_matrix_col =  dates_to_index[report_date] \n",
    "        reporting_matrix[reporting_matrix_row,reporting_matrix_col] = int(cases)\n",
    "    except:\n",
    "        pass\n",
    "np.set_printoptions(suppress=True)    #np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "\n",
    "D=26\n",
    "n_t_d = np.zeros((len(reporting_matrix),D))\n",
    "\n",
    "for i in range(len(reporting_matrix)):\n",
    "    for j in range(i,i+D):\n",
    "        try:\n",
    "            n_t_d[i,j-i] = reporting_matrix[i][j]\n",
    "        except:\n",
    "            pass\n",
    "n_t_d = n_t_d \n",
    "\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "\n",
    "def sim_data(x,y,z):\n",
    "    return n_t_d[:y]\n",
    "\n",
    "\n",
    "\n",
    "pos=[]\n",
    "biweek_x_label = []\n",
    "for year in [\"2014\",\"2015\",\"2016\"]:\n",
    "    for j in np.arange(1,27,13):\n",
    "        if j <= 9:\n",
    "               biweek_x_label.append(year + \"0\"+ str(j))\n",
    "\n",
    "        else:\n",
    "               biweek_x_label.append(year +  str(j))\n",
    "        if year == \"2014\":\n",
    "               pos.append(j)\n",
    "        elif year == \"2015\":\n",
    "               pos.append(j+26)\n",
    "        elif year == \"2016\":\n",
    "               pos.append(j+26*2)\n",
    "\n",
    "from matplotlib.pyplot import cm\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.ylabel(\"Bi-week\")\n",
    "plt.xlabel(\"d\")\n",
    "plt.imshow(n_t_d/np.sum(n_t_d,axis=1),aspect='auto',cmap=cm.Blues)\n",
    "plt.legend()\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(n_t_d.sum(axis=1))\n",
    "plt.xticks(pos, biweek_x_label, rotation='vertical')\n",
    "plt.xlabel(\"\\nDate\")\n",
    "plt.title(\"DHF Incidence in Chiang Mai\")\n",
    "plt.ylabel(\"Incidence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_estimate_prob(po_data,m2,alphas,LO,N_SIM):\n",
    "    ret_arr = []\n",
    "    #print (po_data)\n",
    "    #sys.exit()\n",
    "    phat = alphas/sum(alphas)\n",
    "    for row in np.arange(LO,0,-1):\n",
    "\n",
    "        count = LO\n",
    "        tmp = []\n",
    "        for s_ in range(N_SIM):\n",
    "                p_hat = np.random.dirichlet(alphas,1)[0]\n",
    "                #print (p_hat)\n",
    "                if po_data[D-row] - po_data[D-row-1] >= 100 or m2[s_][(D-row)] <=0:\n",
    "                    tmp_n_t_inf = po_data[D-row]/sum(p_hat[:row])\n",
    "                \n",
    "                elif po_data[D-row] == 0:\n",
    "                    \n",
    "                    tmp_n_t_inf = m2[s_][(D-row)]\n",
    "                else:                        \n",
    "                    tmp_n_t_inf = .1*m2[s_][(D-row)] + .9*po_data[D-row]/sum(p_hat[:row])\n",
    "              \n",
    "                \n",
    "\n",
    "                tmp.append(np.random.normal(tmp_n_t_inf, 10,size=100))\n",
    "                #ret_arr.append(po_data[D-row]/sum(phat[:row]))\n",
    "        count -=1\n",
    "        ret_arr.append(tmp)\n",
    "    return ret_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[8.647445999999999, 31.72372999999999, 9.097832036539089]\n",
      "(array([11., 20.]), array([-24., -20.]), array([-6., 35.]), 8.0)\n",
      "(array([ 8., 16.]), array([-40., -36.]), array([-9., 31.]), 6.0)\n",
      "(array([16., 35.]), array([39., 43.]), array([ 4., 46.]), 9.0)\n",
      "(array([12., 42.]), array([-34., -30.]), array([-1., 48.]), 9.0)\n",
      "(array([0., 0.]), array([-8., -5.]), array([-19.,  20.]), 5.0)\n",
      "(0.0, 0.0, 1.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[4.8024999999999896, 27.812766, 5.42100992818704]\n",
      "(array([ 8., 13.]), array([-30., -27.]), array([-10.,  30.]), 6.0)\n",
      "(array([13., 24.]), array([48., 52.]), array([ 0., 41.]), 9.0)\n",
      "(array([13., 27.]), array([-26., -22.]), array([-2., 39.]), 9.0)\n",
      "(array([3., 8.]), array([-4.,  0.]), array([-15.,  24.]), 5.0)\n",
      "(array([0., 0.]), array([-23., -19.]), array([-20.,  20.]), 2.0)\n",
      "(0.2, 0.0, 1.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[5.1130940000000065, 21.610074000000004, 5.917458637697274]\n",
      "(array([12., 22.]), array([58., 62.]), array([ 0., 40.]), 9.0)\n",
      "(array([11., 21.]), array([-18., -14.]), array([-5., 36.]), 9.0)\n",
      "(array([ 7., 16.]), array([3., 7.]), array([-10.,  31.]), 5.0)\n",
      "(array([3., 8.]), array([-19., -15.]), array([-15.,  24.]), 2.0)\n",
      "(array([0., 0.]), array([-10.,  -6.]), array([-20.,  20.]), 4.0)\n",
      "(0.0, 0.2, 1.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[3.626575999999995, 24.096526000000004, 3.621664826841511]\n",
      "(array([11., 19.]), array([-29., -25.]), array([-6., 34.]), 9.0)\n",
      "(array([ 6., 12.]), array([-7., -3.]), array([-11.,  28.]), 5.0)\n",
      "(array([4., 8.]), array([-29., -25.]), array([-14.,  25.]), 2.0)\n",
      "(array([0., 0.]), array([-17., -13.]), array([-20.,  20.]), 4.0)\n",
      "(array([0., 0.]), array([-27., -23.]), array([-20.,  20.]), 2.0)\n",
      "(0.0, 0.0, 1.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[3.988338, 13.658977999999994, 3.9488365373992105]\n",
      "(array([ 6., 10.]), array([3., 7.]), array([-12.,  27.]), 5.0)\n",
      "(array([3., 6.]), array([-19., -15.]), array([-15.,  24.]), 2.0)\n",
      "(array([4., 7.]), array([-8., -4.]), array([-15.,  25.]), 4.0)\n",
      "(array([0., 0.]), array([-20., -16.]), array([-19.,  20.]), 2.0)\n",
      "(array([0., 0.]), array([-9., -5.]), array([-20.,  19.]), 12.0)\n",
      "(0.2, 0.2, 1.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[8.218802000000005, 17.614014, 8.242891693043262]\n",
      "(array([3., 5.]), array([-17., -13.]), array([-16.,  23.]), 2.0)\n",
      "(array([3., 5.]), array([-6., -3.]), array([-16.,  23.]), 4.0)\n",
      "(array([2., 3.]), array([-19., -16.]), array([-17.,  22.]), 2.0)\n",
      "(array([20., 48.]), array([-9., -5.]), array([ 7., 56.]), 12.0)\n",
      "(array([0., 0.]), array([-5., -2.]), array([-19.,  20.]), 21.0)\n",
      "(0.4, 0.0, 0.8)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[17.712514, 12.485091999999998, 4.126304817546499]\n",
      "(array([6., 8.]), array([-2.,  2.]), array([-13.,  26.]), 4.0)\n",
      "(array([3., 5.]), array([-16., -12.]), array([-16.,  23.]), 2.0)\n",
      "(array([18., 30.]), array([-5., -1.]), array([ 2., 43.]), 12.0)\n",
      "(array([16., 34.]), array([-4.,  0.]), array([ 2., 45.]), 21.0)\n",
      "(array([0., 0.]), array([66., 70.]), array([48., 87.]), 72.0)\n",
      "(0.2, 0.0, 1.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[25.091972000000016, 34.44127599999998, 25.316937911590863]\n",
      "(array([3., 4.]), array([-14., -10.]), array([-17.,  23.]), 2.0)\n",
      "(array([15., 21.]), array([-4., -0.]), array([-2., 37.]), 12.0)\n",
      "(array([16., 23.]), array([-2.,  2.]), array([-2., 38.]), 21.0)\n",
      "(array([50., 89.]), array([66., 70.]), array([41., 93.]), 72.0)\n",
      "(array([0., 0.]), array([-10.,  -6.]), array([-20.,  20.]), 111.0)\n",
      "(0.4, 0.0, 0.8)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[41.12210799999999, 64.45219599999999, 39.774552751337964]\n",
      "(array([15., 19.]), array([-0.,  4.]), array([-4., 35.]), 12.0)\n",
      "(array([22., 28.]), array([1., 5.]), array([ 3., 42.]), 21.0)\n",
      "(array([ 81., 106.]), array([69., 73.]), array([ 67., 113.]), 72.0)\n",
      "(array([100., 145.]), array([-9., -5.]), array([ 90., 152.]), 111.0)\n",
      "(array([0., 0.]), array([-10.,  -6.]), array([-20.,  19.]), 168.0)\n",
      "(0.2, 0.2, 0.8)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[86.61296399999999, 109.456116, 85.93547604121272]\n",
      "(array([21., 24.]), array([5., 9.]), array([ 1., 40.]), 21.0)\n",
      "(array([ 89., 113.]), array([72., 76.]), array([ 75., 120.]), 72.0)\n",
      "(array([ 86., 110.]), array([-6., -2.]), array([ 75., 121.]), 111.0)\n",
      "(array([0., 0.]), array([-8., -4.]), array([-20.,  20.]), 168.0)\n",
      "(array([0., 0.]), array([-21., -17.]), array([-20.,  20.]), 222.0)\n",
      "(0.2, 0.2, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[103.68764000000006, 164.49986399999997, 103.06884567565999]\n",
      "(array([76., 85.]), array([76., 80.]), array([ 60., 100.]), 72.0)\n",
      "(array([129., 151.]), array([-3.,  1.]), array([113., 161.]), 111.0)\n",
      "(array([274., 375.]), array([-6., -2.]), array([269., 380.]), 168.0)\n",
      "(array([217., 320.]), array([-19., -16.]), array([211., 317.]), 222.0)\n",
      "(array([0., 0.]), array([-2.,  2.]), array([-19.,  20.]), 293.0)\n",
      "(0.2, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[154.90758599999998, 212.32203799999996, 146.42303425043542]\n",
      "(array([113., 125.]), array([-3.,  0.]), array([ 97., 139.]), 111.0)\n",
      "(array([150., 170.]), array([-6., -2.]), array([137., 182.]), 168.0)\n",
      "(array([348., 438.]), array([-20., -15.]), array([343., 439.]), 222.0)\n",
      "(array([516., 763.]), array([-2.,  2.]), array([478., 746.]), 293.0)\n",
      "(array([0., 0.]), array([10., 14.]), array([-8., 31.]), 256.0)\n",
      "(0.2, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[107.98559199999993, 221.77868400000003, 81.80372440512177]\n",
      "(array([143., 155.]), array([28., 31.]), array([117., 157.]), 168.0)\n",
      "(array([232., 255.]), array([ 8., 12.]), array([197., 242.]), 222.0)\n",
      "(array([410., 486.]), array([21., 25.]), array([365., 447.]), 293.0)\n",
      "(array([254., 330.]), array([23., 27.]), array([225., 309.]), 256.0)\n",
      "(array([0., 0.]), array([55., 59.]), array([37., 76.]), 314.0)\n",
      "(0.2, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[118.41001399999985, 247.468634, 100.60245581738714]\n",
      "(array([216., 231.]), array([46., 50.]), array([184., 226.]), 222.0)\n",
      "(array([302., 332.]), array([51., 55.]), array([266., 313.]), 293.0)\n",
      "(array([320., 368.]), array([49., 52.]), array([285., 345.]), 256.0)\n",
      "(array([353., 443.]), array([71., 74.]), array([317., 412.]), 314.0)\n",
      "(array([0., 0.]), array([24., 27.]), array([ 6., 45.]), 402.0)\n",
      "(0.2, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[138.39105600000002, 252.80695599999999, 109.2466497586494]\n",
      "(array([293., 313.]), array([120., 124.]), array([262., 305.]), 293.0)\n",
      "(array([265., 288.]), array([109., 113.]), array([237., 281.]), 256.0)\n",
      "(array([365., 417.]), array([123., 127.]), array([332., 394.]), 314.0)\n",
      "(array([463., 568.]), array([63., 67.]), array([416., 526.]), 402.0)\n",
      "(array([0., 0.]), array([56., 60.]), array([38., 78.]), 480.0)\n",
      "(0.2, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[404.66627000000017, 209.34045200000008, 355.08731454213455]\n",
      "(array([271., 288.]), array([226., 230.]), array([253., 296.]), 256.0)\n",
      "(array([328., 354.]), array([236., 240.]), array([307., 353.]), 314.0)\n",
      "(array([550., 621.]), array([172., 176.]), array([504., 578.]), 402.0)\n",
      "(array([ 829., 1015.]), array([151., 155.]), array([763., 942.]), 480.0)\n",
      "(array([ 841., 4169.]), array([130., 133.]), array([ 742., 4057.]), 519.0)\n",
      "(0.0, 0.0, 0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[193.09565600000016, 266.73223199999995, 160.61418719713333]\n",
      "(array([335., 354.]), array([260., 264.]), array([314., 357.]), 314.0)\n",
      "(array([472., 508.]), array([198., 202.]), array([435., 485.]), 402.0)\n",
      "(array([646., 730.]), array([177., 180.]), array([593., 675.]), 480.0)\n",
      "(array([ 842., 1028.]), array([154., 157.]), array([766., 939.]), 519.0)\n",
      "(array([130., 641.]), array([111., 115.]), array([129., 629.]), 528.0)\n",
      "(0.2, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[178.43907399999986, 266.0937359999999, 136.70952956773334]\n",
      "(array([441., 467.]), array([253., 257.]), array([410., 456.]), 402.0)\n",
      "(array([507., 544.]), array([238., 242.]), array([471., 522.]), 480.0)\n",
      "(array([703., 787.]), array([214., 218.]), array([651., 734.]), 519.0)\n",
      "(array([813., 991.]), array([170., 174.]), array([743., 898.]), 528.0)\n",
      "(array([ 266., 1389.]), array([130., 134.]), array([ 256., 1303.]), 416.0)\n",
      "(0.2, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[145.0636519999999, 111.13971600000022, 121.37567671265356]\n",
      "(array([513., 541.]), array([385., 389.]), array([489., 537.]), 480.0)\n",
      "(array([590., 633.]), array([383., 387.]), array([561., 617.]), 519.0)\n",
      "(array([623., 694.]), array([347., 352.]), array([588., 662.]), 528.0)\n",
      "(array([630., 753.]), array([311., 315.]), array([592., 714.]), 416.0)\n",
      "(array([ 70., 388.]), array([300., 304.]), array([ 91., 377.]), 350.0)\n",
      "(0.2, 0.0, 0.2)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[366.59804, 54.30419800000027, 324.6655556178823]\n",
      "(array([551., 581.]), array([495., 499.]), array([534., 583.]), 519.0)\n",
      "(array([521., 558.]), array([486., 490.]), array([507., 559.]), 528.0)\n",
      "(array([526., 585.]), array([468., 472.]), array([512., 580.]), 416.0)\n",
      "(array([563., 680.]), array([472., 476.]), array([549., 659.]), 350.0)\n",
      "(array([ 782., 3940.]), array([447., 451.]), array([ 762., 3698.]), 481.0)\n",
      "(0.2, 0.0, 0.2)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[153.0076040000002, 119.35987600000007, 92.53659547040733]\n",
      "(array([500., 529.]), array([550., 554.]), array([494., 541.]), 528.0)\n",
      "(array([415., 446.]), array([549., 553.]), array([418., 466.]), 416.0)\n",
      "(array([423., 473.]), array([567., 571.]), array([431., 489.]), 350.0)\n",
      "(array([605., 728.]), array([556., 560.]), array([599., 719.]), 481.0)\n",
      "(array([0., 0.]), array([594., 598.]), array([577., 616.]), 454.0)\n",
      "(0.4, 0.0, 0.2)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[121.37355000000002, 98.5982379999998, 112.05417766818175]\n",
      "(array([390., 412.]), array([499., 503.]), array([388., 432.]), 416.0)\n",
      "(array([339., 362.]), array([506., 510.]), array([343., 388.]), 350.0)\n",
      "(array([647., 719.]), array([486., 490.]), array([640., 722.]), 481.0)\n",
      "(array([571., 690.]), array([514., 518.]), array([560., 674.]), 454.0)\n",
      "(array([ 77., 383.]), array([566., 569.]), array([121., 365.]), 386.0)\n",
      "(0.2, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[136.56526000000002, 109.10224600000001, 153.6723281608884]\n",
      "(array([317., 336.]), array([430., 434.]), array([315., 357.]), 350.0)\n",
      "(array([506., 543.]), array([392., 396.]), array([497., 549.]), 481.0)\n",
      "(array([324., 360.]), array([407., 411.]), array([323., 374.]), 454.0)\n",
      "(array([3., 3.]), array([448., 452.]), array([28., 67.]), 386.0)\n",
      "(array([0., 0.]), array([386., 390.]), array([368., 408.]), 120.0)\n",
      "(0.0, 0.0, 0.2)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[71.05101399999987, 150.41144600000013, 114.1046340502835]\n",
      "(array([488., 514.]), array([380., 384.]), array([476., 524.]), 481.0)\n",
      "(array([509., 542.]), array([386., 390.]), array([487., 537.]), 454.0)\n",
      "(array([561., 621.]), array([423., 427.]), array([540., 611.]), 386.0)\n",
      "(array([ 84., 102.]), array([361., 365.]), array([ 99., 142.]), 120.0)\n",
      "(array([0., 0.]), array([338., 342.]), array([320., 359.]), 34.0)\n",
      "(0.0, 0.0, 0.4)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[41.63036600000026, 271.26204400000006, 128.6775840725783]\n",
      "(array([477., 503.]), array([462., 466.]), array([464., 510.]), 454.0)\n",
      "(array([441., 471.]), array([512., 516.]), array([438., 485.]), 386.0)\n",
      "(array([48., 53.]), array([457., 461.]), array([ 71., 111.]), 120.0)\n",
      "(array([27., 33.]), array([441., 445.]), array([51., 91.]), 34.0)\n",
      "(array([0., 0.]), array([498., 502.]), array([480., 519.]), 30.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[37.98390399999988, 375.47316199999995, 135.95374294504526]\n",
      "(array([414., 438.]), array([528., 532.]), array([414., 458.]), 386.0)\n",
      "(array([38., 40.]), array([476., 480.]), array([ 63., 103.]), 120.0)\n",
      "(array([51., 56.]), array([462., 465.]), array([ 75., 114.]), 34.0)\n",
      "(array([57., 68.]), array([519., 523.]), array([ 88., 129.]), 30.0)\n",
      "(array([0., 0.]), array([470., 474.]), array([452., 492.]), 18.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[29.329877999999987, 338.6826680000002, 112.33819043590067]\n",
      "(array([35., 38.]), array([391., 395.]), array([53., 92.]), 120.0)\n",
      "(array([40., 43.]), array([365., 369.]), array([54., 94.]), 34.0)\n",
      "(array([44., 49.]), array([414., 418.]), array([ 64., 103.]), 30.0)\n",
      "(array([44., 52.]), array([358., 362.]), array([59., 99.]), 18.0)\n",
      "(array([0., 0.]), array([366., 370.]), array([348., 388.]), 9.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[8.921662000000005, 82.57750399999996, 24.218849641362517]\n",
      "(array([38., 40.]), array([108., 112.]), array([26., 66.]), 34.0)\n",
      "(array([35., 38.]), array([155., 159.]), array([29., 68.]), 30.0)\n",
      "(array([26., 29.]), array([86., 90.]), array([14., 53.]), 18.0)\n",
      "(array([19., 23.]), array([91., 95.]), array([ 8., 48.]), 9.0)\n",
      "(array([0., 0.]), array([66., 70.]), array([48., 88.]), 12.0)\n",
      "(0.0, 0.0, 0.8)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[12.965316000000016, 34.17570599999998, 9.907150256009448]\n",
      "(array([33., 35.]), array([110., 114.]), array([22., 62.]), 30.0)\n",
      "(array([21., 22.]), array([45., 49.]), array([ 4., 44.]), 18.0)\n",
      "(array([11., 12.]), array([48., 52.]), array([-4., 35.]), 9.0)\n",
      "(array([33., 39.]), array([26., 30.]), array([15., 55.]), 12.0)\n",
      "(array([0., 0.]), array([27., 31.]), array([ 9., 48.]), 31.0)\n",
      "(0.0, 0.2, 0.8)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[11.306011999999997, 28.579988000000004, 11.424016633336938]\n",
      "(array([19., 21.]), array([-8., -4.]), array([ 0., 40.]), 18.0)\n",
      "(array([9., 9.]), array([1., 5.]), array([-12.,  28.]), 9.0)\n",
      "(array([18., 20.]), array([-20., -16.]), array([-0., 39.]), 12.0)\n",
      "(array([0., 0.]), array([-16., -12.]), array([-20.,  20.]), 31.0)\n",
      "(array([0., 0.]), array([-25., -21.]), array([-20.,  20.]), 16.0)\n",
      "(0.2, 0.0, 0.8)\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "variance_level_results = []\n",
    "import sys\n",
    "from pydlm import dlm, trend, seasonality, dynamic, autoReg, longSeason\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pyramid.arima import ARIMA\n",
    "\n",
    "import os\n",
    "\n",
    "rmse_vec_cv = []\n",
    "#with suppress_stdout():\n",
    "rmse_vec = []\n",
    "sim_data_var = []\n",
    "N_SIM = 1000\n",
    "sim_results_pi = []\n",
    "sim_results_mse = []\n",
    "for sim_num in np.arange(30,60):\n",
    "            sim_n_t_d = sim_data(D,sim_num,False)\n",
    "            train = sim_n_t_d\n",
    "            \n",
    "\n",
    "\n",
    "            train = np.array(train)\n",
    "            train_n_t_d = train.reshape((-1,D))\n",
    "            ts = train_n_t_d.sum(axis=1)\n",
    "            data_to_be_scaled_down  = train_n_t_d[len(ts)-D:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            delayed_data = []\n",
    "            count = D\n",
    "            for i in range(len(data_to_be_scaled_down)):\n",
    "                tmp = data_to_be_scaled_down[i][:count].tolist()\n",
    "                while len(tmp) <D:\n",
    "                    tmp.append(0)\n",
    "                delayed_data.append(tmp)\n",
    "                count -=1 \n",
    "\n",
    "\n",
    "            training_data = np.append(train_n_t_d[:len(ts)-D],delayed_data,axis=0)\n",
    "\n",
    "            k = np.array(train_n_t_d).shape[1 ]\n",
    "            alphas = np.ones(k)\n",
    "\n",
    "            for i in range(len(ts)-D):\n",
    "                alphas += train_n_t_d[i]\n",
    "\n",
    "            \n",
    "           \n",
    "            #######\n",
    "            # MODEL 1: Delay\n",
    "            ########\n",
    "            delay_model_samples = []\n",
    "            for s_ in range(N_SIM):\n",
    "                model_1_delay = []\n",
    "                count = D\n",
    "                p_vec_noise = np.random.dirichlet(alphas)\n",
    "                for i in range(len(delayed_data)):\n",
    "                    delay_forecast = np.sum(delayed_data[i])/np.sum(p_vec_noise[:count])\n",
    "                    model_1_delay.append(np.round(delay_forecast,2))\n",
    "                    count -= 1\n",
    "                delay_model_samples.append(model_1_delay)\n",
    "            \n",
    "            delay_model_samples = np.array(delay_model_samples)\n",
    "            \n",
    "\n",
    "            #######\n",
    "            # MODEL 2 : Forecast\n",
    "            ########\n",
    "            \n",
    "            LO=5\n",
    "            process_training_data = np.append(ts[:len(ts)-D],model_1_delay[:D-LO],axis=0)\n",
    "            process_training_data = np.append(proc_training_data_base,process_training_data,axis=0)\n",
    "            from pydlm import dlm, trend, seasonality, dynamic, autoReg, longSeason\n",
    "            myDLM = dlm(process_training_data)\n",
    "            myDLM = myDLM + seasonality(26, name='7day', w=.5)\n",
    "            myDLM = myDLM + autoReg(degree=3, data=process_training_data, name='ar2', w=1.0)\n",
    "            myDLM.fit()\n",
    "            (process_model_forecast, predictVar) = myDLM.predictN(N=LO, date=myDLM.n-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #stepwise_model = auto_arima(process_training_data, start_p=1, start_q=1,\n",
    "            #               max_p=3, max_q=3, m=26,\n",
    "            #               start_P=0, seasonal=True,\n",
    "            #               d=1, D=1, trace=True,\n",
    "            ##               error_action='ignore',  \n",
    "             #              suppress_warnings=True, \n",
    "             #              stepwise=True)\n",
    "            #process_model_forecast = stepwise_model.predict(n_periods=LO)\n",
    "           \n",
    "            predictVar = 1*np.ones(len(process_model_forecast))\n",
    "            #######\n",
    "            # MODEL 2\n",
    "            ########\n",
    "            forecast_model_samples = []\n",
    "            for s_ in range(N_SIM):\n",
    "                model_2_delay = []\n",
    "                count = D\n",
    "                for i in np.arange(LO,0,-1):\n",
    "                    tmp = np.random.normal(process_model_forecast[LO-i],np.sqrt(predictVar[LO-i]))\n",
    "                    model_2_delay.append(np.round(tmp,2))\n",
    "                    count -= 1\n",
    "\n",
    "                forecast_model_samples.append(np.append(model_1_delay[:D-LO],model_2_delay))\n",
    "            \n",
    "            forecast_model_samples = np.array(forecast_model_samples)\n",
    "            \n",
    "                \n",
    "            model_average = bayes_estimate_prob(np.sum(delayed_data,axis=1),forecast_model_samples,alphas,LO,N_SIM)\n",
    "            \n",
    "            delay_model_samples = np.transpose(np.array(delay_model_samples))\n",
    "            forecast_model_samples = np.transpose(np.array(forecast_model_samples))\n",
    "            \n",
    "            delay_sim_res = delay_model_samples\n",
    "            fcast_sim_res = forecast_model_samples\n",
    "            avg_sim_res = model_average\n",
    "            LO_av = np.array(avg_sim_res).reshape((LO,-1))\n",
    "            LO_delay = delay_sim_res[D-LO:]\n",
    "            LO_fcast = fcast_sim_res[D-LO:]\n",
    "            LO_truth = ts[len(ts)-LO:]\n",
    "            #print (np.mean(LO_delay,axis=1),np.mean(LO_fcast,axis=1),LO_truth\\\n",
    "                   #,np.mean(LO_av,axis=1))\n",
    "            #sys.exit()\n",
    "            sim_results_mse.append([mean_absolute_error(np.mean(LO_delay,axis=1),LO_truth),\\\n",
    "                                   mean_absolute_error(np.mean(LO_fcast,axis=1),LO_truth),\\\n",
    "                                   mean_absolute_error(np.mean(LO_av,axis=1),LO_truth)])\n",
    "            print (sim_results_mse[-1])\n",
    "            av_cp = 0\n",
    "            fcast_cp = 0\n",
    "            delay_cp = 0\n",
    "            for i in range(LO):\n",
    "                LO_av_ci = np.round(np.percentile(LO_av[i],[2.5,97.5]))\n",
    "                LO_fcast_ci = np.round(np.percentile(LO_fcast[i],[2.5,97.5]))\n",
    "                LO_delay_ci = np.round(np.percentile(LO_delay[i],[2.5,97.5]))\n",
    "                print (LO_delay_ci,LO_fcast_ci,LO_av_ci, LO_truth[i])\n",
    "                if LO_av_ci[0] <= LO_truth[i] <= LO_av_ci[1]:\n",
    "                    av_cp +=1\n",
    "\n",
    "                if LO_fcast_ci[0] <= LO_truth[i] <= LO_fcast_ci[1]:\n",
    "                    fcast_cp +=1\n",
    "\n",
    "                if LO_delay_ci[0] <= LO_truth[i] <= LO_delay_ci[1]:\n",
    "                    delay_cp +=1\n",
    "\n",
    "\n",
    "            av_cp = 1.*av_cp/LO\n",
    "            fcast_cp = 1.*fcast_cp/LO\n",
    "            delay_cp = 1.*delay_cp/LO\n",
    "\n",
    "            print (delay_cp,fcast_cp,av_cp)\n",
    "            sim_results_pi.append([delay_cp,fcast_cp,av_cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14666667 0.03333333 0.53333333]\n",
      "[[  8.647446    31.72373      9.09783204]\n",
      " [  4.8025      27.812766     5.42100993]\n",
      " [  5.113094    21.610074     5.91745864]\n",
      " [  3.626576    24.096526     3.62166483]\n",
      " [  3.988338    13.658978     3.94883654]\n",
      " [  8.218802    17.614014     8.24289169]\n",
      " [ 17.712514    12.485092     4.12630482]\n",
      " [ 25.091972    34.441276    25.31693791]\n",
      " [ 41.122108    64.452196    39.77455275]\n",
      " [ 86.612964   109.456116    85.93547604]\n",
      " [103.68764    164.499864   103.06884568]\n",
      " [154.907586   212.322038   146.42303425]\n",
      " [107.985592   221.778684    81.80372441]\n",
      " [118.410014   247.468634   100.60245582]\n",
      " [138.391056   252.806956   109.24664976]\n",
      " [404.66627    209.340452   355.08731454]\n",
      " [193.095656   266.732232   160.6141872 ]\n",
      " [178.439074   266.093736   136.70952957]\n",
      " [145.063652   111.139716   121.37567671]\n",
      " [366.59804     54.304198   324.66555562]\n",
      " [153.007604   119.359876    92.53659547]\n",
      " [121.37355     98.598238   112.05417767]\n",
      " [136.56526    109.102246   153.67232816]\n",
      " [ 71.051014   150.411446   114.10463405]\n",
      " [ 41.630366   271.262044   128.67758407]\n",
      " [ 37.983904   375.473162   135.95374295]\n",
      " [ 29.329878   338.682668   112.33819044]\n",
      " [  8.921662    82.577504    24.21884964]\n",
      " [ 12.965316    34.175706     9.90715026]\n",
      " [ 11.306012    28.579988    11.42401663]]\n",
      "[ 91. 132.  91.]\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "print np.mean(sim_results_pi,axis=0)\n",
    "sim_results_mse = np.array(sim_results_mse)\n",
    "print (sim_results_mse)\n",
    "\n",
    "print np.round(np.mean(sim_results_mse,axis=0))\n",
    "\n",
    "wins = 0\n",
    "for i in range(len(sim_results_mse)):\n",
    "    if sim_results_mse[i][2] < sim_results_mse[i][0]:\n",
    "        wins +=1\n",
    "        \n",
    "print (1.*wins/(len(sim_results_mse)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 78, 26)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gcgibson/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[325627.8522973311, 31451.487937133505, 23743.425214048777]\n",
      "(array([1189., 1346.]), array([-198.,  209.]), array([ -0., 147.]), 287.0)\n",
      "(array([0., 0.]), array([-196.,  207.]), array([ -0., 147.]), 0.0)\n",
      "(array([0., 0.]), array([-213.,  218.]), array([ -0., 147.]), 0.0)\n",
      "(array([ 960., 1261.]), array([-209.,  210.]), array([ -0., 147.]), 278.0)\n",
      "(array([0., 0.]), array([-199.,  212.]), array([ -0., 147.]), 1.0)\n",
      "(0.4, 0.6, 0.6)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[15459.1863900645, 31923.139478738594, 27066.49213265992]\n",
      "(array([266., 303.]), array([-245.,  228.]), array([ -0., 136.]), 287.0)\n",
      "(array([0., 0.]), array([-216.,  252.]), array([ -0., 137.]), 0.0)\n",
      "(array([0., 0.]), array([-225.,  226.]), array([ -0., 137.]), 0.0)\n",
      "(array([0., 0.]), array([-249.,  245.]), array([ -0., 136.]), 278.0)\n",
      "(array([0., 0.]), array([-227.,  220.]), array([ -0., 137.]), 1.0)\n",
      "(0.6, 0.6, 0.6)\n",
      "Starting forward filtering...\n",
      "Forward fitering completed.\n",
      "Starting backward smoothing...\n",
      "Backward smoothing completed.\n",
      "[964653.4454847556, 16896.904534580357, 12685.474454565436]\n",
      "(array([2076., 2340.]), array([-205.,  217.]), array([ -0., 278.]), 28.0)\n",
      "(array([0., 0.]), array([-220.,  223.]), array([ -0., 278.]), 0.0)\n",
      "(array([0., 0.]), array([-217.,  228.]), array([ -0., 278.]), 0.0)\n",
      "(array([0., 0.]), array([-229.,  216.]), array([ -0., 278.]), 287.0)\n",
      "(array([35., 46.]), array([-226.,  213.]), array([ -0., 278.]), 0.0)\n",
      "(0.4, 0.8, 0.8)\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as pltq\n",
    "\n",
    "variance_level_results = []\n",
    "import sys\n",
    "from pydlm import dlm, trend, seasonality, dynamic, autoReg, longSeason\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "cv_mse = []\n",
    "season_data = []\n",
    "sim_results_mse = []\n",
    "sim_results_pi = []\n",
    "for i in range(3):\n",
    "    season_data.append(reporting_matrix[i:(i+26)])\n",
    "    \n",
    "season_data = np.array(season_data).reshape((3,-1,26))\n",
    "\n",
    "print (season_data.shape)\n",
    "\n",
    "for season_for_leave_out in np.arange(3):\n",
    "        sim_n_t_d = season_data#[:cutoff]\n",
    "\n",
    "        train = [sim_n_t_d[x] for x in range(3) if x not in [season_for_leave_out]]\n",
    "        test = sim_n_t_d[season_for_leave_out]\n",
    "        \n",
    "        train = np.array(train)\n",
    "        train_n_t_d = train.reshape((-1,D))\n",
    "        ts = train_n_t_d.sum(axis=1)\n",
    "\n",
    "        data_to_be_scaled_down  = test[len(test)-D:]\n",
    "       \n",
    "        delayed_data = []\n",
    "        count = D\n",
    "        for i in range(len(data_to_be_scaled_down)):\n",
    "            tmp = data_to_be_scaled_down[i][:count].tolist()\n",
    "            while len(tmp) <D:\n",
    "                tmp.append(0)\n",
    "            delayed_data.append(tmp)\n",
    "            count -=1 \n",
    "\n",
    "\n",
    "        training_data = np.append(train_n_t_d[:len(ts)-D],delayed_data,axis=0)\n",
    "\n",
    "        k = np.array(train_n_t_d).shape[1 ]\n",
    "        alphas = np.ones(k)\n",
    "\n",
    "        for i in range(len(ts)-D):\n",
    "            alphas += train_n_t_d[i]\n",
    "\n",
    "\n",
    "\n",
    "        #######\n",
    "        # MODEL 1: Delay\n",
    "        ########\n",
    "        delay_model_samples = []\n",
    "        for s_ in range(N_SIM):\n",
    "            model_1_delay = []\n",
    "            count = D\n",
    "            p_vec_noise = np.random.dirichlet(alphas)\n",
    "            for i in range(len(delayed_data)):\n",
    "                delay_forecast = np.sum(delayed_data[i])/np.sum(p_vec_noise[:count])\n",
    "                model_1_delay.append(np.round(delay_forecast,2))\n",
    "                count -= 1\n",
    "            delay_model_samples.append(model_1_delay)\n",
    "\n",
    "        delay_model_samples = np.array(delay_model_samples)\n",
    "\n",
    "\n",
    "        #######\n",
    "        # MODEL 2 : Forecast\n",
    "        ########\n",
    "\n",
    "        LO=5\n",
    "        process_training_data = np.append(ts[:len(ts)-D],model_1_delay[:D-LO],axis=0)\n",
    "        from pydlm import dlm, trend, seasonality, dynamic, autoReg, longSeason\n",
    "        myDLM = dlm(process_training_data)\n",
    "       # myDLM = myDLM + seasonality(26, name='7day', w=1.0)\n",
    "        myDLM = myDLM + autoReg(degree=2, data=process_training_data, name='ar2', w=1.0)\n",
    "        myDLM.fit()\n",
    "        (process_model_forecast, predictVar) = myDLM.predictN(N=LO, date=myDLM.n-1)\n",
    "\n",
    "        #######\n",
    "        # MODEL 2\n",
    "        ########\n",
    "        forecast_model_samples = []\n",
    "        for s_ in range(N_SIM):\n",
    "            model_2_delay = []\n",
    "            count = D\n",
    "            for i in np.arange(LO,0,-1):\n",
    "                tmp = np.random.normal(process_model_forecast[LO-i],np.sqrt(predictVar[LO-i]))\n",
    "                model_2_delay.append(np.round(tmp,2))\n",
    "                count -= 1\n",
    "\n",
    "            forecast_model_samples.append(np.append(model_1_delay[:D-LO],model_2_delay))\n",
    "\n",
    "        forecast_model_samples = np.array(forecast_model_samples)\n",
    "\n",
    "\n",
    "        model_average = bayes_estimate_prob(np.sum(delayed_data,axis=1),forecast_model_samples,alphas,LO,N_SIM)\n",
    "\n",
    "        model_average = np.transpose(np.array(model_average).reshape((-1,LO)))\n",
    "        delay_model_samples = np.transpose(np.array(delay_model_samples))\n",
    "        forecast_model_samples = np.transpose(np.array(forecast_model_samples))\n",
    "\n",
    "        delay_sim_res = delay_model_samples\n",
    "        fcast_sim_res = forecast_model_samples\n",
    "        avg_sim_res = model_average\n",
    "        LO_av = avg_sim_res\n",
    "        LO_delay = delay_sim_res[D-LO:]\n",
    "        LO_fcast = fcast_sim_res[D-LO:]\n",
    "        LO_truth = ts[len(ts)-LO:]\n",
    "\n",
    "        sim_results_mse.append([mean_squared_error(np.mean(LO_delay,axis=1),LO_truth),\\\n",
    "                               mean_squared_error(np.mean(LO_fcast,axis=1),LO_truth),\\\n",
    "                               mean_squared_error(np.mean(LO_av,axis=1),LO_truth)])\n",
    "        print (sim_results_mse[-1])\n",
    "        av_cp = 0\n",
    "        fcast_cp = 0\n",
    "        delay_cp = 0\n",
    "        for i in range(LO):\n",
    "            LO_av_ci = np.round(np.percentile(LO_av[i],[2.5,97.5]))\n",
    "            LO_fcast_ci = np.round(np.percentile(LO_fcast[i],[2.5,97.5]))\n",
    "            LO_delay_ci = np.round(np.percentile(LO_delay[i],[2.5,97.5]))\n",
    "            print (LO_delay_ci,LO_fcast_ci,LO_av_ci, LO_truth[i])\n",
    "            if LO_av_ci[0] <= LO_truth[i] <= LO_av_ci[1]:\n",
    "                av_cp +=1\n",
    "\n",
    "            if LO_fcast_ci[0] <= LO_truth[i] <= LO_fcast_ci[1]:\n",
    "                fcast_cp +=1\n",
    "\n",
    "            if LO_delay_ci[0] <= LO_truth[i] <= LO_delay_ci[1]:\n",
    "                delay_cp +=1\n",
    "\n",
    "\n",
    "        av_cp = 1.*av_cp/LO\n",
    "        fcast_cp = 1.*fcast_cp/LO\n",
    "        delay_cp = 1.*delay_cp/LO\n",
    "\n",
    "        print (delay_cp,fcast_cp,av_cp)\n",
    "        sim_results_pi.append([delay_cp,fcast_cp,av_cp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46666667 0.66666667 0.66666667]\n",
      "[435247.  26757.  21165.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### sim_results_pi = np.array(sim_results_pi)#### sim \n",
    "print np.mean(sim_results_pi,axis=0)\n",
    "\n",
    "sim_results_mse = np.array(sim_results_mse)\n",
    "print np.round(np.mean(sim_results_mse,axis=0)/1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
