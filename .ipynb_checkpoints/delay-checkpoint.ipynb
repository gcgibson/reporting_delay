{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to forecast using estimates from the delay distribution we develop the following notation. \n",
    "\n",
    "\n",
    "Suppose we have a time series $Y_1,Y_2,...,Y_n$ indexed by a sequence of time values $t_1,t_2,...,t_n$. We adopt the forecasting notation of Reich et. al (2018) in order to evaluate targets across models. \n",
    "\n",
    "Suppose we are interested in forecasting $Y_j$ at time $t_j$. Because of the delay nature of the data we only observe **complete** data up till time $t_j - L$ for a fixed L. \n",
    "\n",
    "\n",
    "For example, if $t_j$ is \"now\" then $t_j - L$ is the last $t^*$ for which all possible delay values $t^*,t^*+1, ....,+ t^*+L$ are completely observed. \n",
    "\n",
    "We define the following prediction $z(t_j - L +k | t_j)$ to be the *k-step* ahead prediction relative to time $t_j$. We define the MSE of model $M$ to be \n",
    "\n",
    "$$MSE_{t_j}(M) = \\frac{1}{L}\\sum_{k=0}^L [z(t_j - L +k | t_j) - Y_{j-L+k}]^2$$\n",
    "\n",
    "In order to compute an average MSE over the full model $M$ we compute \n",
    "\n",
    "$$MSE(M) = \\frac{1}{|T|}\\sum_{t_j \\in T}MSE_{t_j}(M)$$\n",
    "\n",
    "\n",
    "For this experiment we examine 2 models \n",
    "\n",
    "$$M_1 = \\text{ process model ignorning delay }$$\n",
    "\n",
    "$$M_2 = \\text{ process model weighted by delay distribution }$$\n",
    "\n",
    "\n",
    "We have $51$ total points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 20)\n",
      "(51,)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  0.  2.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  3.  2.  2.  0.  3.  0.  0.  1.  2.  1.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  3.  1.  3.  1.  0.\n",
      "   1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  4.  1.  3.  4.  2.  0.  0.  4.  1.  1.  1.  0.  2.\n",
      "   2.  0.]\n",
      " [ 0.  0.  0.  0.  7.  2.  5.  1.  0.  0.  0.  6.  1.  0.  1.  0.  0.  2.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  5.  3.  3.  1.  0.  0.  0.  2.  1.  2.  2.  1.  1.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0. 14. 14.  8.  3.  4.  0.  0.  2.  0.  2.  1.  0.  2.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0. 16. 14.  5.  3.  6.  0.  0.  3.  3.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 5.  7. 13.  5. 10.  0.  0.  3.  1.  3.  1.  0.  1.  2.  0.  0.  3.  0.\n",
      "   0.  0.]\n",
      " [ 5.  7.  9.  9.  0.  0.  4.  4.  4.  0.  1.  0.  1.  0.  0.  3.  0.  0.\n",
      "   0.  0.]\n",
      " [ 9.  5.  6.  0.  0.  5.  4.  4.  2.  3.  4.  2.  0.  0.  2.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 4.  6.  0.  0.  2. 11.  5.  2.  2.  6.  0.  0.  0.  5.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 7.  0.  0.  1. 10.  7.  3.  3.  1.  3.  0.  0.  4.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  8.  4.  3.  4.  2.  3.  0.  0.  4.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  2.  8. 13.  3.  3.  4.  3.  0.  0.  4.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 9.  3.  8.  4.  7.  7.  2.  0.  0.  4.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 5.  1.  9.  9.  6.  5.  0.  0.  7.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 7.  5.  9. 10.  6.  0.  0.  6.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 6.  9. 11. 11.  0.  0.  7.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [10.  7.  7.  0.  0. 12.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 5. 12.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 2.  0.  0.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  6.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  1.  0.  4.  5.  0.  2.  2.\n",
      "   3.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  5.  1.  1.  2.  1.  3.\n",
      "   1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  5.  1.  3.  0.  2.  4.  0.  4.  0.  3.  1.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  5.  2.  2.  0.  5. 10.  0.  2.  1.  3.  0.  1.\n",
      "   1.  3.]\n",
      " [ 0.  0.  0.  0.  0.  6.  4.  2.  0.  6.  6.  0.  2.  2.  3.  1.  1.  0.\n",
      "   3.  1.]\n",
      " [ 0.  0.  0.  0.  6.  7.  5.  0.  4.  6.  1.  0.  1.  1.  0.  1.  1.  1.\n",
      "   1.  1.]\n",
      " [ 0.  0.  0.  8.  7.  6.  0.  9.  6.  1.  1.  4.  1.  1.  2.  2.  3.  1.\n",
      "   3.  0.]\n",
      " [ 0.  0. 14.  5. 14.  0.  6.  7.  1.  6.  2.  3.  3.  3.  1.  4.  0.  3.\n",
      "   0.  0.]\n",
      " [ 0.  8. 11.  9.  0. 13.  5.  2.  4.  5.  1.  2.  2.  3.  1.  0.  1.  0.\n",
      "   0.  0.]\n",
      " [ 6.  4.  4.  0.  6.  6.  8.  4.  4.  5.  4.  2.  1.  1.  0.  4.  0.  0.\n",
      "   0.  0.]\n",
      " [ 9.  3.  0.  1.  8.  6.  6.  6.  8.  5.  2.  3.  3.  0.  4.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 2.  0.  0. 13.  5.  6.  5.  7.  6.  5.  4.  3.  2.  4.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  4.  7. 15.  4. 13.  4.  2.  3.  6.  1.  2.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 5.  0.  1.  5.  7.  5.  5.  2.  5.  5.  2.  4.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 8.  0.  4. 12. 11.  7.  4.  9.  5.  2.  2.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 4.  2.  2. 16. 11.  6.  4.  6.  3.  3.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 9.  3.  0. 17.  5.  5. 12.  5.  6.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 8.  1.  4.  9.  2. 11.  3.  7.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 6.  0.  4.  5.  6.  7.  4.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 4.  2.  4. 11. 11. 14.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [10.  0.  1.  7.  9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 3.  2.  4. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 6.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 9.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat Jan  6 17:12:30 2018\n",
    "\n",
    "@author: gcgibson\n",
    "\"\"\"\n",
    "\n",
    "import scipy.stats\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.random import random\n",
    "import math\n",
    "from pydlm import dlm, trend, seasonality, dynamic, autoReg, longSeason\n",
    "\n",
    "def observation_function(time_series_at_t,t,D,particle,params):\n",
    "    \n",
    "    tmp =  scipy.stats.poisson.pmf(time_series_at_t,np.exp(particle))\n",
    "    if math.isnan(tmp):\n",
    "        tmp = 0\n",
    "    return tmp\n",
    "\n",
    "def transition_function(particles,params):\n",
    "    particles[:,0]  += np.random.normal(0,1,len(particles))\n",
    "    return particles\n",
    "\n",
    "def expected_value_observation_function(p):\n",
    "    return np.exp(p)\n",
    "\n",
    "def expected_value_transition_function(p):\n",
    "    return p\n",
    "\n",
    "def create_uniform_particles( N,state_space_dimension):\n",
    "    particles  = np.random.normal(0,1 , size=(N,state_space_dimension))\n",
    "    return particles\n",
    "\n",
    "def predict(particles,t,params):\n",
    "    particles = transition_function(particles,params)\n",
    "    return particles\n",
    "\n",
    "\n",
    "def update(particles, weights,ts,t,D,params):\n",
    "    weights.fill(1.)\n",
    "    for p in range(len(particles)):\n",
    "        weights[p] *= observation_function(ts[t],t,D,particles[p],params)\n",
    "    weights += 1.e-300\n",
    "    return weights/sum(weights)  \n",
    "\n",
    "\n",
    "\n",
    "def neff(weights):\n",
    "    return 1. / np.sum(np.square(weights))\n",
    "\n",
    "\n",
    "def estimate(particles, weights):\n",
    "    \"\"\"returns mean and variance of the weighted particles\"\"\"\n",
    "    pos = particles[:, 0]\n",
    "    mean = np.average(pos, weights=weights, axis=0)\n",
    "    var  = np.average((pos - mean)**2, weights=weights, axis=0)\n",
    "    return mean, var\n",
    "\n",
    "### VARIOUS RESAMPLING SCHEMES\n",
    "def multinomal_resample(weights):\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "    cumulative_sum[-1] = 1.  # avoid round-off errors\n",
    "    return np.searchsorted(cumulative_sum, random(len(weights)))\n",
    "\n",
    "\n",
    "def resample_from_index(particles, weights, indexes):\n",
    "    particles[:] = particles[indexes]\n",
    "    weights[:] = weights[indexes]\n",
    "    weights.fill(1.0 / len(weights))\n",
    "    return particles,weights\n",
    "\n",
    "def stratified_resample(weights):\n",
    "    N = len(weights)\n",
    "    # make N subdivisions, chose a random position within each one\n",
    "    positions = (random(N) + range(N)) / N\n",
    "\n",
    "    indexes = np.zeros(N, 'i')\n",
    "    cumulative_sum = np.cumsum(weights)\n",
    "\n",
    "    i, j = 0, 0\n",
    "    while i < N:\n",
    "        if positions[i] < cumulative_sum[j]:\n",
    "            indexes[i] = j\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    return indexes\n",
    "\n",
    "\n",
    "\n",
    "def run_pf(time_series,N,state_space_dimension,D,params):\n",
    "    \n",
    "    particles = create_uniform_particles(N=N,state_space_dimension=state_space_dimension)\n",
    "    weights = np.zeros(N)    \n",
    "    xs = [] \n",
    "    ws = []\n",
    "    ws.append(weights)\n",
    "    for t in range(len(time_series)):\n",
    "        particles = predict(particles,t,params)       \n",
    "        # incorporate measurements\n",
    "        weights = update(particles, weights,time_series, t, D, params)\n",
    "        ws.append(weights)\n",
    "        #print (neff(weights),time_series[t],params)\n",
    "        indexes = stratified_resample(weights)\n",
    "        particles,weights = resample_from_index(particles, weights, indexes)\n",
    "        mu, var = estimate(particles, weights)\n",
    "        xs.append(mu.tolist())\n",
    "    return xs,particles,ws\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Suppose we are given data in the form \n",
    "\n",
    "n_{0,0} , n_{0,1} , n_{0,2}, n_{0,3}\n",
    "n_{1,0} , n_{1,1} , n_{1,2}, 0\n",
    "n_{2,0} , n_{2,1} , 0      , 0\n",
    "n_{3,0} , 0       , 0      , 0\n",
    "\n",
    "where T=3.\n",
    "\n",
    "Now suppose D=2, then we truncate this matrix as follows\n",
    "\n",
    "n_{0,0} , n_{0,1} , n_{0,2}, 0\n",
    "n_{1,0} , n_{1,1} , n_{1,2}, 0\n",
    "n_{2,0} , n_{2,1} , 0      , 0\n",
    "n_{3,0} , 0       , 0      , 0\n",
    "\n",
    "\n",
    "so for a setting of parameters (T=2,D=3) the reporting trapezoid is completely \n",
    "defined\n",
    "\n",
    "In order to get the N_{t,T}s we simply add up the rows\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SIMULATE = False\n",
    "D = 20\n",
    "if SIMULATE == False:\n",
    "    n_t_d = []\n",
    "    with open(\"province-biweek_with_delays.csv\") as f:\n",
    "        i = 0\n",
    "        for line in f.readlines():\n",
    "            if i > 0:\n",
    "                n_t_d.append(line.replace(\"\\n\",\"\").split(','))\n",
    "            i+=1\n",
    "    date_to_index = {}\n",
    "    \n",
    "    i = 0\n",
    "    for elm in n_t_d:\n",
    "        if len(elm[1]) == 1:\n",
    "            elm[1] = \"0\" + elm[1]\n",
    "        date_to_index[elm[0]+elm[1]] = i\n",
    "    \n",
    "        i+=1\n",
    "    \n",
    "    d_to_i = {}\n",
    "    i = 0\n",
    "    iter_ =  date_to_index.keys()\n",
    "    iter_.sort()\n",
    "    for key in iter_:\n",
    "    \n",
    "        d_to_i[key] = i\n",
    "        i+=1\n",
    "    \n",
    "    n_t = np.zeros((52-1,52-1))\n",
    "    \n",
    "    for elm in n_t_d:\n",
    "        try:\n",
    "            \n",
    "            sick_date = d_to_i[elm[0]+elm[1]]\n",
    "            report_date = d_to_i[elm[4] + elm[5]]\n",
    "            if int(elm[4] + elm[5]) < 201621 and int(elm[3]) == 1:\n",
    "                n_t[sick_date][report_date] += int(elm[3])\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    n_t_d = []\n",
    "    for row in range(len(n_t)):\n",
    "        #if len(n_t[row][row:row+D]) == D:\n",
    "            tmp = n_t[row][row:row+D].tolist()\n",
    "            while len(tmp) < D:\n",
    "                tmp += [0]\n",
    "            n_t_d.append(tmp)\n",
    "    \n",
    "    n_t_d = np.array(n_t_d)\n",
    "    \n",
    "    n_t_inf = np.sum(n_t_d,axis=1)\n",
    "\n",
    "else:\n",
    "    true_p = [.1,.3,.2,.15,.15,.1]\n",
    "    n_t_d = []\n",
    "    n_t_inf = []\n",
    "    states = []\n",
    "    tmp = np.random.normal(4,1 ,1)\n",
    "    states.append(tmp)\n",
    "    n_t_inf.append(np.random.poisson(np.exp(tmp))[0])\n",
    "    for i in range(1,10):\n",
    "        tmp = np.random.normal(states[i-1],1,1)\n",
    "        states.append(tmp)\n",
    "        n_t_inf.append(np.random.poisson(np.exp(tmp))[0])\n",
    "    for i in range(len(n_t_inf)):\n",
    "        n_t_d.append(np.random.multinomial(n_t_inf[i],true_p).tolist())\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "total_mse_m1 = 0\n",
    "total_mse_m2 = 0\n",
    "\n",
    "print (n_t_d.shape)\n",
    "\n",
    "## 51x10\n",
    "print (n_t_inf.shape)\n",
    "\n",
    "## 51\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "print (n_t_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "T = range(len(n_t_d)-10,len(n_t_d))\n",
    "for tau in T:\n",
    "\n",
    "\n",
    "    train_n_t_d = n_t_d[:tau-D + 1]\n",
    "    train_n_t_inf = n_t_inf[:tau-D + 1]\n",
    "    #print (test_n_t_inf.shape)\n",
    "    count = D\n",
    "    for t_prime in range(len(train_n_t_d)-D,len(train_n_t_d)):\n",
    "        train_n_t_d[t_prime][count:] = 0\n",
    "        count -=1\n",
    "    \n",
    "    test_n_t_d = n_t_d[tau- D + 1:tau ]\n",
    "    test_n_t_inf = n_t_inf[tau-D +1:tau ]\n",
    "    \n",
    "\n",
    "    import numpy as np\n",
    "    import pymc3 as pm\n",
    "    import pandas as pd\n",
    "\n",
    "\n",
    "    ## Delay Model\n",
    "\n",
    "    DELAY_DIST = True\n",
    "    if DELAY_DIST == True:\n",
    "\n",
    "        k = np.array(train_n_t_d).shape[1 ]\n",
    "\n",
    "        with pm.Model() as multinom_test:\n",
    "            a = pm.Dirichlet('a', a=np.ones(k))\n",
    "            for i in range(len(train_n_t_d)):\n",
    "                data_pred = pm.Multinomial('data_pred_%s'% i, n=sum(train_n_t_d[i]), p=a, observed=train_n_t_d[i])\n",
    "            trace = pm.sample(50000, pm.Metropolis())\n",
    "            #trace = pm.sample(1000) # also works with NUTS\n",
    "\n",
    "        pm.traceplot(trace[500:]);\n",
    "\n",
    "    state_trajectories = []\n",
    "    PF = False\n",
    "    if  PF:\n",
    "        N = 10000\n",
    "        state_space_dimension = 1\n",
    "\n",
    "        params = []\n",
    "        means , particles, weights = run_pf(train_n_t_inf,N,state_space_dimension,D,params)\n",
    "\n",
    "\n",
    "\n",
    "        ### Interval Predictions\n",
    "        state_trajectories = [particles]\n",
    "        observation_trajectories = [np.exp(particles)]\n",
    "        for i in range(len(test_n_t_inf)):\n",
    "            tmp = expected_value_transition_function(state_trajectories[i-1])\n",
    "            observation_trajectories.append(expected_value_observation_function(tmp))\n",
    "            state_trajectories.append(tmp) \n",
    "\n",
    "        state_trajectories = state_trajectories[1:]\n",
    "        ## MEAN\n",
    "        #print (np.mean(observation_trajectories,axis=1))\n",
    "        ## QUANTILES \n",
    "        state_trajectories = np.array(state_trajectories).reshape((len(test_n_t_inf),-1))\n",
    "\n",
    "\n",
    "    else:\n",
    "        myDLM = dlm(train_n_t_inf)\n",
    "        myDLM = myDLM + trend(1, name='lineTrend', w=1.0)\n",
    "        # add a 7 day seasonality with prior covariance 1.0\n",
    "        myDLM = myDLM + seasonality(52, name='7day', w=1.0)\n",
    "        # add a 3 step auto regression\n",
    "        myDLM = myDLM + autoReg(degree=2, data=train_n_t_inf, name='ar3', w=1.0)\n",
    "        myDLM.fit()\n",
    "        (predictMean, predictVar) = myDLM.predictN(N=D-1, date=myDLM.n-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(predictMean)):\n",
    "        samples = np.random.normal(predictMean[i],np.sqrt(predictVar[i]),100)\n",
    "        state_trajectories.append(samples)\n",
    "    state_trajectories = np.array(state_trajectories)\n",
    "\n",
    "\n",
    "    phat = trace['a'].mean(axis=0)\n",
    "    from scipy.stats import binom\n",
    "\n",
    "\n",
    "\n",
    "    myDLM.plot()\n",
    "\n",
    "    ##compute weighted trajectories \n",
    "\n",
    "    weighted_trajectories = []\n",
    "    for i in range(len(state_trajectories)):\n",
    "        tmp = []\n",
    "        samples = state_trajectories[i]\n",
    "        row_sum = sum(test_n_t_d[i])\n",
    "        q = sum(phat[:len(phat)-i-1])\n",
    "        for samp in samples:\n",
    "            btemp = binom.pmf(row_sum,samp,q)\n",
    "            if np.isnan(btemp):\n",
    "                tmp.append(0)\n",
    "            else:\n",
    "                tmp.append(btemp)\n",
    "            #print (row_sum,samp,q,btemp)\n",
    "        weighted_trajectories.append(tmp)\n",
    "    weighted_trajectories = np.array(weighted_trajectories)\n",
    "\n",
    "    \n",
    "    for i in range(len(weighted_trajectories)):\n",
    "        weighted_trajectories[i] = weighted_trajectories[i]/sum(weighted_trajectories[i])\n",
    "    where_are_NaNs = np.isnan(weighted_trajectories)\n",
    "    weighted_trajectories[where_are_NaNs] = 1e-50\n",
    "    #print (weighted_trajectories)\n",
    "    ###\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "    max_indeces = np.argmax(weighted_trajectories,axis=1)\n",
    "    max_point = []\n",
    "    for i in range(len(max_indeces)):\n",
    "        max_point.append(state_trajectories[i][max_indeces[i]])\n",
    "\n",
    "\n",
    "    print \"MSE ignoring delay\"\n",
    "    print (mean_squared_error(np.average(state_trajectories,axis=1),test_n_t_inf))\n",
    "    total_mse_m1 += mean_squared_error(np.average(state_trajectories,axis=1),test_n_t_inf)\n",
    "    print \"MSE delay adjusted\"\n",
    "    print (mean_squared_error(np.average(state_trajectories,weights = weighted_trajectories,axis=1),test_n_t_inf))\n",
    "    total_mse_m2 += mean_squared_error(np.average(state_trajectories,weights = weighted_trajectories,axis=1),test_n_t_inf)\n",
    "    print \"MSE taking most likely trajectory\"\n",
    "    print (mean_squared_error(max_point,test_n_t_inf))\n",
    "\n",
    "\n",
    "print \"--------------\\n\\n\\n\"\n",
    "print \"MSE ignoring delay\"\n",
    "print (total_mse_m1)\n",
    "print \"MSE delay adjusted\"\n",
    "print (total_mse_m2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
